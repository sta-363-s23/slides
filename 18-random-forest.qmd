---
title: "Random Forests"
author: "Lucy D'Agostino McGowan"
footer:  "[Dr. Lucy D'Agostino McGowan](https://lucymcgowan.com) <i>adapted from slides by Hastie & Tibshirani</i>"
logo: "img/icon.png"
format: 
  kakashi-revealjs: 
    theme: [custom.scss]
    slide-number: true
    chalkboard: true
    title-slide-attributes: 
      data-background-color: "#fff" 
      data-color: "#70001A"
    menu: true
editor: 
  markdown: 
    wrap: 72
---


## Random forests {.small}

```{r child = "setup.Rmd"}
```


```{r, include = FALSE}
library(tidyverse)
library(ISLR)
library(tidymodels)
set.seed(1)
```


_Do you_ `r emo::ji("red_heart")` _all of the tree puns?_

* Random forests provide an improvement over bagged trees
by way of a small tweak that _decorrelates_ the trees


* By _decorrelating_ the trees, this reduces the variance even more when we average the trees!



## Random Forest process {.small}

* Like bagging, build a number of decision trees on
bootstrapped training samples


* Each time the tree is split, instead of considering _all predictors_ (like bagging), **a random selection of** $m$ **predictors** is chosen as split candidates from the full set of $p$ predictors
* The split is allowed to use only one of those $m$ predictors


* A fresh selection of $m$ predictors is taken at each split 


* typically we choose $m \approx \sqrt{p}$




## <i class="fas fa-edit"></i> `Choosing m for Random Forest`

Let's say you have a dataset with 100 observations and 9 variables, if you were fitting a random forest, what would a good $m$ be?

```{r}
#| echo: false 
countdown::countdown(minutes = 1)
```


# The heart disease example 

_Recall that we are predicting whether a patient has heart disease from 13 predictors_


## 1. Randomly divide the data in half, 149 training observations, 148 testing {.small}

```{r, echo = FALSE, message = FALSE, warning = FALSE}
heart <- read_csv("data/heart.csv")
heart$HD <- as.factor(heart$AHD)
heart <- heart[complete.cases(heart), ]
```

```{r}
set.seed(77)
heart_split <- initial_split(heart, prop = 0.5)
heart_train <- training(heart_split)
```



## 2. Create model specification {.small}

```{r, eval = FALSE}
model_spec <- rand_forest(
  mode = "classification",
  mtry = ---
) |> 
  set_engine("ranger")
```


. . .

::: question
`mtry` here is `m`. If we are doing _bagging_ what do you think we set this to? 
:::



## 2. Create bagging specification

```{r, eval = FALSE}
bagging_spec <- rand_forest(
  mode = "classification",
  mtry = 13 #<<
) |> 
  set_engine("ranger")
```



::: question
What would we change `mtry` to if we are doing a random forest?
:::



## 2. Create Random Forest specification

```{r}
rf_spec <- rand_forest(
  mode = "classification",
  mtry = 3 #<<
) |> 
  set_engine("ranger")
```



* The default for `rand_forest` is `floor(sqrt(# predictors))` (so 3 in this case)

## 3. Create the workflow {.small}

```{r}
wf <- workflow() |>
  add_recipe(
    recipe(
      HD ~ Age + Sex + ChestPain + RestBP + Chol + Fbs + 
               RestECG + MaxHR + ExAng + Oldpeak + Slope + Ca + Thal,
             data = heart_train
    )
  ) |>
  add_model(rf_spec)
```


## 4. Fit the model

```{r}
model <- fit(wf, data = heart_train)
```



## 5. Examine how it looks in the test data {.small}

```{r}
heart_test <- testing(heart_split)
model |>
  predict(new_data = heart_test) |>
  bind_cols(heart_test) |>
  conf_mat(truth = HD, estimate = .pred_class) |>
  autoplot(type = "heatmap")
```



## <i class="fas fa-laptop"></i> `Application Exercise`

* Open your last application exercise
* Refit your model as a _random forest_

```{r}
#| echo: false
countdown::countdown(minutes = 10)
```



