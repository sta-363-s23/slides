---
title: "Ridge Regression"
footer:  "[Dr. Lucy D'Agostino McGowan](https://lucymcgowan.com)"
logo: "img/icon.png"
format: 
  kakashi-revealjs: 
    theme: [custom.scss]
    slide-number: true
    chalkboard: true
    title-slide-attributes: 
      data-background-color: "#fff" 
      data-color: "#70001A"
    menu: true
---


```{r, echo = FALSE, message = FALSE, warning = FALSE}
knitr::opts_chunk$set(message = FALSE, warning = FALSE)
library(tidyverse)
library(broom)
library(ISLR)
library(countdown)
```



## Linear Regression Review {.small}

::: question
In linear regression, what are we minimizing? How can I write this in matrix form?
:::


* RSS!
* $(\mathbf{y} - \mathbf{X}\hat\beta)^T(\mathbf{y}-\mathbf{X}\hat\beta)$
* What is the solution ( $\hat\beta$ ) to this?
* $\mathbf{(X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{y}$



## Linear Regression Review {.small}

::: question
What is $\mathbf{X}$?
:::

- the design matrix!



## Linear Regression Review {.small}

::: question
How did we get $\mathbf{(X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{y}$?
:::

$$
\begin{align}
-2\mathbf{X}^T\mathbf{y}+2\mathbf{X}^T\mathbf{X}\hat\beta &= 0\\
2\mathbf{X}^T\mathbf{X}\hat\beta & = 2\mathbf{X}^T\mathbf{y} \\
\mathbf{X}^T\mathbf{X}\hat\beta & =\mathbf{X}^T\mathbf{y} \\
(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{X}\hat\beta &=(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{y}\\
\underbrace{(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{X}}_{\mathbf{I}}\hat\beta &=(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{y}\\
\mathbf{I}\hat\beta &= (\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{y}\\
\hat\beta & = (\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{y}
\end{align}
$$




## Linear Regression Review


Let's try to find an $\mathbf{X}$ for which it would be impossible to calculate $\hat\beta$




## Calculating in R {.small}

y | x
---|---
4 | 1
3 | 2
1 | 5
3 | 1
5 | 5



## Creating a vector in R {.small}


y | x
---|---
4 | 1
3 | 2
1 | 5
3 | 1
5 | 5



```{r}
y <- c(4, 3, 1, 3, 5)
```




## Creating a Design matrix in R {.small}


y | x
---|---
4 | 1
3 | 2
1 | 5
3 | 1
5 | 5


```{r}
(X <- matrix(c(rep(1, 5), 
               c(1, 2, 5, 1, 5)),
             ncol = 2))
```




## Taking a transpose in R {.small}

```{r}
t(X)
```



## Taking an inverse in R {.small}

```{r}
XTX <- t(X) %*% X
solve(XTX)
```



## Put it all together {.small}

```{r}
solve(t(X) %*% X) %*% t(X) %*% y
```




## <i class="fas fa-laptop"></i> `Application Exercise`

In R, find a design matrix `X` where it is not possible to calculate $\hat\beta$ 

```{r, eval = FALSE}
solve(t(X) %*% X) %*% t(X) %*% y
```


```{r}
#| echo: false
countdown(minutes = 5)
```



## Estimating $\hat\beta$ {.small}


$\hat\beta = \mathbf{(X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{y}$

::: question
Under what circumstances is this equation not estimable?
:::



* when we can't invert $(\mathbf{X^TX})^{-1}$
* $p > n$
* multicollinearity
* A guaranteed way to check whether a square matrix is not invertible is to check whether the **determinant** is equal to zero





## Estimating $\hat\beta$ {.small}

$$\mathbf{X} = \begin{bmatrix}1 & 2 & 3 & 1 \\ 1 & 3 & 4& 0 \end{bmatrix}$$

::: question
What is $n$ here? What is $p$?
:::

## Estimating $\hat\beta$ {.small}

$$\mathbf{X} = \begin{bmatrix}1 & 2 & 3 & 1 \\ 1 & 3 & 4& 0 \end{bmatrix}$$


::: question
Is $\mathbf{(X^TX)^{-1}}$ going to be invertible? 
:::

. . .

```{r, eval = FALSE}
X <- matrix(c(1, 1, 2, 3, 3, 4, 1, 0), nrow = 2)
det(t(X) %*% X)
```

```{r, echo = FALSE}
X <- matrix(c(1, 1, 2, 3, 3, 4, 1, 0), nrow = 2)
round(det(t(X) %*% X))
```



## Estimating $\hat\beta$ {.small}

$$\mathbf{X} = \begin{bmatrix} 1 & 3 & 6 \\ 1 & 4 & 8 \\1 & 5& 10\\ 1 & 2 & 4\end{bmatrix}$$


::: question
Is $\mathbf{(X^TX)^{-1}}$ going to be invertible? 
:::

. . .

```{r}
X <- matrix(c(1, 1, 1, 1, 3, 4, 5, 2, 6, 8, 10, 4), nrow = 4)
det(t(X) %*% X)
cor(X[, 2], X[, 3])
```




## Estimating $\hat\beta$ {.small}

$$\mathbf{X} = \begin{bmatrix} 1 & 3 & 6 \\ 1 & 4 & 8 \\1 & 5& 10\\ 1 & 2 & 4\end{bmatrix}$$


::: question
What was the problem this time?
:::



```{r}
X <- matrix(c(1, 1, 1, 1, 3, 4, 5, 2, 6, 8, 10, 4), nrow = 4)
det(t(X) %*% X)
cor(X[, 2], X[, 3])
```





## Estimating $\hat\beta$ {.small}

::: question
What is a sure-fire way to tell whether $\mathbf{(X^TX)^{-1}}$ will be invertible?
:::

* Take the determinant!
* $|\mathbf{A}|$ means the determinant of matrix $\mathbf{A}$
* For a 2x2 matrix:
* $\mathbf{A} = \begin{bmatrix}a&b\\c&d\end{bmatrix}$
* $|\mathbf{A}| = ad - bc$



## Estimating $\hat\beta$ {.small}

::: question
What is a sure-fire way to tell whether $\mathbf{(X^TX)^{-1}}$ will be invertible?
:::


* Take the determinant!

$|\mathbf{A}|$ means the determinant of matrix $\mathbf{A}$

* For a 3x3 matrix:
* $\mathbf{A} = \begin{bmatrix}a&b&c\\d&e&f\\g&h&i\end{bmatrix}$
* $|\mathbf{A}| = a(ei-fh)-b(di-fg) +c(dh-eg)$



## Determinants {.small}

_It looks funky, but it follows a nice pattern!_

$$\mathbf{A} = \begin{bmatrix}a&b&c\\d&e&f\\g&h&i\end{bmatrix}$$
$$|\mathbf{A}| = a(ei-fh)-b(di-fg) +c(dh-eg)$$


* (1) multiply $a$ by the determinant of the portion of the matrix that are **not** in $a$'s row or column
* do the same for $b$ (2) and $c$ (3)
* put it together as **plus** (1) **minus** (2) **plus** (3)

. . .

$$|\mathbf{A}| = a \left|\begin{matrix}e&f\\h&i\end{matrix}\right|-b\left|\begin{matrix}d&f\\g&i\end{matrix}\right|+c\left|\begin{matrix}d&e\\g&h\end{matrix}\right|$$




## <i class="fas fa-laptop"></i> `Application Exercise`

Calculate the determinant of the following matrices in R using the `det()` function:

$$\mathbf{A} = \begin{bmatrix} 1 & 2 \\ 4 & 5 \end{bmatrix}$$

$$\mathbf{B} = \begin{bmatrix} 1 & 2 & 3 \\ 3 & 6 & 9 \\ 2 & 5 & 7\end{bmatrix}$$
Are these both invertible?

```{r} 
#| echo: false
countdown(minutes = 1)
```



## Estimating $\hat\beta$ {.small}

$$\mathbf{X} = \begin{bmatrix} 1 & 3.01 & 6 \\ 1 & 4 & 8 \\1 & 5& 10\\ 1 & 2 & 4\end{bmatrix}$$


::: question
Is $\mathbf{(X^TX)^{-1}}$ going to be invertible? 
:::


```{r}
X <- matrix(c(1, 1, 1, 1, 3.01, 4, 5, 2, 6, 8, 10, 4), nrow = 4)
det(t(X) %*% X)
cor(X[, 2], X[, 3])
```




## Estimating $\hat\beta$ {.small}

$$\mathbf{X} = \begin{bmatrix} 1 & 3.01 & 6 \\ 1 & 4 & 8 \\1 & 5& 10\\ 1 & 2 & 4\end{bmatrix}$$


::: question
Is $\mathbf{(X^TX)^{-1}}$ going to be invertible? 
:::


```{r}
y <- c(1, 2, 3, 2)
solve(t(X) %*% X) %*% t(X) %*% y
```




## Estimating $\hat\beta$ {.small}

$$\mathbf{X} = \begin{bmatrix} 1 & 3.01 & 6 \\ 1 & 4 & 8 \\1 & 5& 10\\ 1 & 2 & 4\end{bmatrix}$$


::: question
Is $\mathbf{(X^TX)^{-1}}$ going to be invertible? 
:::

$$\begin{bmatrix}\hat\beta_0\\\hat\beta_1\\\hat\beta_2\end{bmatrix} = \begin{bmatrix}1.28\\-114.29\\57.29\end{bmatrix}$$



## Estimating $\hat\beta$ {.small}

$$\mathbf{X} = \begin{bmatrix} 1 & 3.01 & 6 \\ 1 & 4 & 8 \\1 & 5& 10\\ 1 & 2 & 4\end{bmatrix}$$


::: question
What is the equation for the variance of $\hat\beta$?
:::

$$var(\hat\beta) = \hat\sigma^2(\mathbf{X}^T\mathbf{X})^{-1}$$

* $\hat\sigma^2 = \frac{RSS}{n-(p+1)}$

## Variance of $\hat\beta$

$$var(\hat\beta) = \begin{bmatrix} \mathbf{0.91835}   &-24.489  &  12.132\\-24.48943  & \mathbf{4081.571} & -2038.745 \\12.13247 & -2038.745  &\mathbf{1018.367}\end{bmatrix}$$




## Estimating $\hat\beta$ {.small}

$$\mathbf{X} = \begin{bmatrix} 1 & 3.01 & 6 \\ 1 & 4 & 8 \\1 & 5& 10\\ 1 & 2 & 4\end{bmatrix}$$



$$var(\hat\beta) = \begin{bmatrix} \mathbf{0.91835}   &-24.489  &  12.132\\-24.48943  & \mathbf{4081.571} & -2038.745 \\12.13247 & -2038.745  &\mathbf{1018.367}\end{bmatrix}$$

::: question
What is the variance for $\hat\beta_0$?
:::



## Estimating $\hat\beta$ {.small}

$$\mathbf{X} = \begin{bmatrix} 1 & 3.01 & 6 \\ 1 & 4 & 8 \\1 & 5& 10\\ 1 & 2 & 4\end{bmatrix}$$



$$var(\hat\beta) = \begin{bmatrix} \color{blue}{\mathbf{0.91835}}&-24.489  &  12.132\\-24.48943  & \mathbf{4081.571} & -2038.745 \\12.13247 & -2038.745  &\mathbf{1018.367}\end{bmatrix}$$

::: question
What is the variance for $\hat\beta_0$?
:::



## Estimating $\hat\beta$ {.small}

$$\mathbf{X} = \begin{bmatrix} 1 & 3.01 & 6 \\ 1 & 4 & 8 \\1 & 5& 10\\ 1 & 2 & 4\end{bmatrix}$$



$$var(\hat\beta) = \begin{bmatrix} \mathbf{0.91835}   &-24.489  &  12.132\\-24.48943  & \mathbf{4081.571} & -2038.745 \\12.13247 & -2038.745  &\mathbf{1018.367}\end{bmatrix}$$

::: question
What is the variance for $\hat\beta_1$?
:::



## Estimating $\hat\beta$ {.small}

$$\mathbf{X} = \begin{bmatrix} 1 & 3.01 & 6 \\ 1 & 4 & 8 \\1 & 5& 10\\ 1 & 2 & 4\end{bmatrix}$$



$$var(\hat\beta) = \begin{bmatrix} \mathbf{0.91835}   &-24.489  &  12.132\\-24.48943  & \color{blue}{\mathbf{4081.571}} & -2038.745 \\12.13247 & -2038.745  &\mathbf{1018.367}\end{bmatrix}$$

::: question
What is the variance for $\hat\beta_1$? `r emo::ji("scream")`
:::



## What's the problem?

* Sometimes we can't solve for $\hat\beta$

::: question
Why?
:::



## What's the problem? {.small}

* Sometimes we can't solve for $\hat\beta$
* $\mathbf{X}^T\mathbf{X}$ is not invertible
* We have more variables than observations ( $p > n$ )
* The variables are linear combinations of one another
* Even when we **can** invert $\mathbf{X}^T\mathbf{X}$, things can go wrong
* The variance can blow up, like we just saw!




# What can we do about this?



## Ridge Regression {.small}

* What if we add an additional _penalty_ to keep the $\hat\beta$ coefficients small (this will keep the variance from blowing up!)
* Instead of minimizing $RSS$, like we do with linear regresion, let's minimize $RSS$ PLUS some penalty function
* $RSS + \underbrace{\lambda\sum_{j=1}^p\beta^2_j}_{\textrm{shrinkage penalty}}$


::: question
What happens when $\lambda=0$? What happens as $\lambda\rightarrow\infty$?
:::



## Ridge Regression {.small}

::: question
Let's solve for the $\hat\beta$ coefficients using Ridge Regression. What are we minimizing?
:::

. . .

$$(\mathbf{y}-\mathbf{X}\beta)^T(\mathbf{y}-\mathbf{X}\beta)+\lambda\beta^T\beta$$





## <i class="fas fa-edit"></i> `Try it!`

Find $\hat\beta$ that minimizes this:

$$(\mathbf{y}-\mathbf{X}\beta)^T(\mathbf{y}-\mathbf{X}\beta)+\lambda\beta^T\beta$$

```{r}
#| echo: false
countdown(minutes = 2)
```



## Ridge Regression {.small}

$$\hat\beta_{ridge} = (\mathbf{X}^T\mathbf{X}+\lambda\mathbf{I})^{-1}\mathbf{X}^T\mathbf{y}$$


* Not only does this help with the variance, it solves our problem when $\mathbf{X}^{T}\mathbf{X}$ isn't invertible!



## Choosing $\lambda$ {.small}

* $\lambda$ is known as a **tuning parameter** and is selected using **cross validation**
* For example, choose the $\lambda$ that results in the smallest estimated test error



## Bias-variance tradeoff {.small}

::: question
How do you think ridge regression fits into the bias-variance tradeoff?
:::


* As $\lambda$ `r emo::ji("point_up")`, bias `r emo::ji("point_up")`, variance `r emo::ji("point_down")`
* Bias( $\hat\beta_{ridge}$ ) = $-\lambda(\mathbf{X}^T\mathbf{X}+\lambda\mathbf{I})^{-1}\beta$

## Bias-variance tradeoff {.small}

::: question
What would this be if $\lambda$ was 0?
:::


* Var( $\hat\beta_{ridge}$ ) = $\sigma^2(\mathbf{X}^T\mathbf{X}+\lambda\mathbf{I})^{-1}\mathbf{X}^T\mathbf{X}(\mathbf{X}^T\mathbf{X}+\lambda\mathbf{I})^{-1}$

## Bias-variance tradeoff {.small}

::: question
Is this bigger or smaller than $\sigma^2(\mathbf{X}^T\mathbf{X})^{-1}$? What is this when $\lambda = 0$? As $\lambda\rightarrow\infty$ does this go up or down? 
:::



## Ridge Regression

* **IMPORTANT**: When doing ridge regression, it is important to standardize your variables (divide by the standard deviation)



::: question
Why?
:::


